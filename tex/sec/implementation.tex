\section{Implementation}
\label{sec:implementation}
	
	\subsection{Choosing a Neighbour}
	\label{ssec:choosing-a-neighbour}
		
		We stated in section \ref{ssec:model-selec} that we want to select a \enquote{good} model for the prediction.
		To this goal, we have to define the functions and parameters of the algorithm.
		The most important one is the $\m{nbr}$ function whose purpose is to choose a neighbour efficiently since the final solution depends on a sequence of neighbours.
		In most cases it is best to select a neighbour not too far away from the given subset. %citation!
		% Quelle: https://en.wikipedia.org/wiki/Simulated_annealing

		Our $\m{nbr}$ function generates a random natural number $r\in\set{1,\ldots,k}$ that represents the index of a measured wavelength.
		If this predictor is already in our current subset then we remove it.
		If not, we include it to the new subset.
		That way, new neighbours are not too far away from the current parameter vector.
		The pseudocode is shown in the following listing.
		\medskip
		\begin{tcolorbox}[colframe=black,colbacktitle=white,coltitle=black, attach boxed title to top center={yshift=-2mm},enhanced, titlerule=0.1pt, boxrule=0.5pt, arc=5pt,title=Listing:\quad $\m{nbr}$ function]
			\input{sec/nbr-function}
		\end{tcolorbox}
		\medskip
	% subsection choosing-a-neighbour
		
	\subsection{Additional Functions}
	\label{ssec:add-func}
	
		All other functions were defined following a standard scheme.
		It follows from \ref{ssec:mallows-C_p} that
		\[
			\m{cost}(M)\define \m{C_p}^{(M)}
		\]
		In most applications $\m{prob}$ is defined in analogy to the transition of a physical system.
		%here we need maybe some elaboration
		% Quelle: https://en.wikipedia.org/wiki/Simulated_annealing
		\[
			\m{prob}(c_0,c_1,T) \define \exp\curvb{\frac{c_0 - c_1}{T}}
		\]
		Details of $\m{temp}$ are not really important as long as it monotonically decreases in the second parameter.
		So let $\alpha\in(0,1)$.
		\[
			\m{temp}(T_0,i,i_\m{max})\define T_0\alpha^i
		\]
	%subsection Additional Functions
	
	\subsection{Preprocessing}
	\label{ssec:preprocessing}
	
		 Implementing the algorithm described in \ref{ssec:model-selec} takes a sizeable toll on computing power.
		 The most expensive calculations are performed in the computation of the residual sum of squares
		 \[
		 	\norm{y - \mathbb{X}^{(M)}\hat{\beta}^{(M)}(y)}^2_2
		 \] 
		 which requires the computation of
		 \[
		 	\hat{\beta}^{(M)}(y) = \inv{\curvb{\transp{\mathbb{X}^{(M)}}\mathbb{X}^{(M)}}}\transp{\mathbb{X}^{(M)}}y
		\]
		
		 Instead of solving this, it is more efficient to solve
		 \[
		 	\transp{\mathbb{X}^{(M)}}\mathbb{X}^{(M)}\hat{\beta}(y) = \transp{\mathbb{X}^{(M)}}y
		\]
		which can be done through QR-decomposition or an adequate alternative.
		
		The design matrices $\mathbb{X}^{(M)}$ are full rank by construction.
		It follows then from the definition of $M$ that we can define an injective, monotone increasing function
		\[
			\delta_M : \set{0,\ldots, \abs{M}-1} \rightarrow \set{0,\ldots, k}
		\]
		that maps the indices of the design matrix $\mathbb{X}^{(M)}$ to the indices of the full design matrix $\mathbb{X}$.
		We then only have to precompute $\transp{\mathbb{X}}\mathbb{X}$ and $\transp{\mathbb{X}}y$ and construct $\transp{\mathbb{X}^{(M)}}\mathbb{X}^{(M)}$ and $\transp{\mathbb{X}^{(M)}}y$ by 
		\begin{alignat*}{3}
		 	\transp{\mathbb{X}^{(M)}}\mathbb{X}^{(M)} &=&&\  \curvb{\angleb{x_{\delta(i)}, x_{\delta(j)}}} \qquad i,j& &\ \in&\  M \\
			\transp{\mathbb{X}^{(M)}}y &=&&\ \curvb{\curvb{\transp{\mathbb{X}}y}_{\delta(i)}} \qquad i& &\ \in&\  M
		\end{alignat*}
		 where $x_{\delta(i)}$ is the $\delta(i)$th column vector of $\mathbb{X}$.
		 
		 As $\curvb{\sigma^2}^{(\overline{\Lambda})}$ and by consequence $\curvb{\hat\sigma^2}^{(\overline{\Lambda})}$ are constant, we can precompute the estimated variance of the complete model as well.

	% subsection Preprocessing
	
		
% section implementation