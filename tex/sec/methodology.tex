\section{Methodology}
\label{sec:methodology}
	
	\subsection{Measured Data}
	\label{ssec:measured-data}
	
		As a single spectrum contains overlapping information, it is necessary to determine both relevant wavelengths and the respective parameters to apply NIRS to practical problems.
		To select wavelengths and determine parameters we use an example data set, which contains $p^\m{(SOC)},p^\m{(N)},\m{pH}$ and wave reflectances of 319 wavelengths ranging from $1400 \unit{nm}$ to $2672 \unit{nm}$ by steps of $4 \unit{nm}$ for 533 samples \cite{don:a}.

		We define $\Lambda$ as the set of all measured wavelengths. The reflectance $\varrho(\lambda)$ of a sample at a wavelength $\lambda \in \Lambda$ is recorded as
		\[
			-\lg \varrho(\lambda) = -\frac{\ln \varrho(\lambda)}{\ln 10}
		\]
		Figure \ref{fig:soil-spec-rnd} shows six randomly chosen soil spectra in a diagram.
		\begin{figure*}
			\centering
			\input{gp/soil-spec-rnd.tex}
			\caption{Six near infrared soil spectra of randomly chosen soil samples obtained from the data set, where $\lambda$ is the wavelength and $\rho(\lambda)$ the corresponding reflectance and each colour refers to one sample}
			\label{fig:soil-spec-rnd}
		\end{figure*}
		
	
	% subsection measured-data

	\subsection{Statistical Model}
	\label{ssec:statistical-model}
	
		Let $n\in\SN$ be the size of the data set and $k\in\SN$ with $k< n$ the number of measured wavelengths. We define
		$\varrho_i$ as the soil spectrum of the $i$th sample for every $i\in\SN,i\leq n$.
		$\lambda_j$ is the $j$th measured wavelength for every $j\in\SN,j\leq k$. We will alternatively refer to these as predictors.%note that we might have to introduce the term predictor at this point
		Then according to section \ref{ssec:measured-data} the measured reflectance values are $x_{ij}$ with
		\[
			x_{ij} \define -\lg \varrho_i(\lambda_j)
		\]
		for every $i,j\in\SN,i\leq n,j\leq k$.
		
	
		We define the measured ASF of $\m{SOC}$ of the $i$th sample for every $i\in\SN,i\leq n$ as $p^\m{(SOC)}_i$ to which we will also refer to as response variable.
		To simplify notation, we then define the $n$-dimensional vector
		\[
			p^\m{(SOC)} \define \curvb{p^\m{(SOC)}_i}
		\]

		The Beer-Lambert law allows us to make assumptions on the relations between soil spectra and the response variable \cite[247-8, 254]{agelet:10a}.
		We saw in section \ref{ssec:nirs} that the logarithmised reflectance can be written as a linear combination of molar concentrations.
		Hence, it seems plausible to assume that an ASF can be represented by a linear combination of logarithmised reflectance values.

		Now let $P^\m{(SOC)}$ be the appropriate random vector to $p^\m{(SOC)}$.
		Then under the above assumption the expected values are given for all $i \in \SN, i \leq n$ by
		\[
			\expect P_i^\m{(SOC)} \define \beta^\m{(SOC)}_0 + \sum_{j = 1}^k{x_{ij}\beta^\m{(SOC)}_j} 
		\]
		which simplifies with an $\mathbb{X} \in \SR^{n \times (k+1)}$, called design matrix, and a parameter vector $\beta^\m{(SOC)} \in \SR^{k+1}$ to
		\[
			\expect P^\m{(SOC)} = \mathbb{X}\beta^\m{(SOC)}
		\]
		
		To capture the stochastic part of $P^\m{(SOC)}$, we extend the model to
		\begin{alignat*}{3}
			&P^\m{(SOC)} = \mathbb{X}\beta^\m{(SOC)} + \varepsilon^\m{(SOC)} \\
			&\expect \varepsilon^\m{(SOC)} = 0, \qquad \cov \varepsilon^\m{(SOC)} = (\sigma^2)^\m{(SOC)} \idmat
		\end{alignat*}
		where $(\sigma^2)^\m{(SOC)} \in (0,\infty)$. Following common practice in physics and chemistry, we further assume that 
		\[
			\varepsilon^\m{(SOC)} \sim \FN \curvb{0,(\sigma^2)^\m{(SOC)}\idmat}
		\]
		This results in the complete model
		\[
			P^\m{(SOC)} \sim \FN \curvb{\mathbb{X}\beta^\m{(SOC)},(\sigma^2)^\m{(SOC)} \idmat}
		\]
		The model for the second response variable $P^\m{(N)}$ is constructed in analogy.
		
		The case for the $\m{pH}$ is slightly different, though. 
		When modelling the corresponding random variable we have to adjust the model as the $\m{pH}$ is a logarithmised molar concentration.
		We therefore have to include this into the expected value of the corresponding random variables
		
		\[
			\expect \m{\overline{pH}}_i \define \beta^\m{\m{(pH)}}_0 + \sum_{j = 1}^k{\ln (x_{ij}) \beta^\m{(pH)}_j} 
		\]
		and denote the corresponding matrix by $\mathbb{X}_{\ln}$.

	
	% subsection statistical-model

	\subsection{Multivariate Linear Regression}
	\label{ssec:mlr}
	
		Multivariate linear regression (MLR) is a statistical method for finding parameters of linear relations between a response variable and a set of predictors.
		These can be reused to predict new responses.
		Let $\mathbb{X}\in\SR^{n\times(k+1)}, n,k\in\SN,k<n$ be the design matrix, $\sigma^2\in(0,\infty)$ and $Y$ be the random vector of a response variable with
		\[
			Y \sim \FN\curvb{\mathbb{X}\beta,\sigma^2\idmat_n}
		\]
		for a certain $\beta\in\SR^{k+1}$
		Then through the maximum-likelihood-method and a correction we get two best unbiased estimators $\hat{\beta},\hat{\sigma}^2$ for $\beta$ and $\sigma^2$
		\begin{alignat*}{3}
			\hat{\beta}(Y) &=&&\ \inv{\curvb{\transp{\mathbb{X}}\mathbb{X}}}\transp{\mathbb{X}}Y \\
			\hat{\sigma}^2(Y) &=&&\ \frac{1}{n-(k+1)}\norm{Y - \mathbb{X}\hat{\beta}(Y)}^2
		\end{alignat*}
		Now let $y \define (y_i)\in\SR^n$ be a realization of $Y$.
		Then we define
		\begin{alignat*}{3}
			\hat{y} &\define&&\ \mathbb{X}\hat{\beta}(y) = \mathbb{X}\inv{\curvb{\transp{\mathbb{X}}\mathbb{X}}}\transp{\mathbb{X}}y \\
			\widehat{{\sigma}^2} &\define&&\ \hat{\sigma}^2(y)
		\end{alignat*}
		%citation

	% subsection mlr

	\subsection{Mallows' $\m{C_p}$}
	\label{ssec:mallows-C_p}
	
		At this point, the model is specified using $k+1 = 320$ predictors for each response variable, using the whole measured spectra for each soil sample.
		This set of predictors is comparatively large, $n-k < k$.
		Further, the reflectances of neighbouring lightwaves are correlated. % citation
		In these circumstances, we might overfit the data, i.e. the variance of our estimated parameters $\hat{\beta}_i(Y)$ might be too large, compromising their usability for future measurements.
		To address this problem, it is sensible to limit each actual model to a \enquote{good} subset of the predictors. Hence, our task becomes to select the best or at least a \enquote{sufficiently} good model $M$ defined by
		\[
			M\subset \Lambda \cup \set{\lambda_0} \definedby \overline{\Lambda}
		\]
		where $\lambda_0$ stands for the intercept.
		We denote the design matrix for each $M$ by $\mathbb{X}^{(M)}$.
		Applying MLR to the new design matrix yields the new estimators
		\begin{alignat*}{3}
			\hat{\beta}^{(M)}(Y) &=&&\ \inv{\curvb{\transp{\mathbb{X}^{(M)}}\mathbb{X}^{(M)}}}\transp{\mathbb{X}^{(M)}}Y \\
			\curvb{\hat{\sigma}^2}^{(M)}(Y) &=&&\ \frac{1}{n-\m{p}}\norm{Y - \mathbb{X}^{(M)}\hat{\beta}^{(M)}(Y)}^2_2,
		\end{alignat*}
		where $\m{p} \in \set{2,\ldots,k}$ corresponds to the number of predictors included in $M$.
		%to follow the logic of our argumentation, we should start with the SPSE
		
		The sum of predicted squared errors ($\m{SPSE}$) is a theoretical criterion to compare the merits of different models.
		The $\m{SPSE}^{(M)}$ measures how well a model does in predicting new responses from new data:
		\[
			\m{SPSE}^{(M)} \define \sum_{i = 1}^n \expect \curvb{Y_{n+i} - \hat{Y}_i^{(M)}}^2
		\]
		which simplifies to % to be elaborated
		\[
			\m{SPSE}^{(M)} = n\sigma^2 + \m{p}\sigma^2
		\]
		Unfortunately, the true $\sigma^2$ is unobservable so that it has to be estimated by
		\[
			\widehat{\m{SPSE}}^{(M)}(Y) \define \norm{Y - \mathbb{X}^{(M)}\hat{\beta}^{(M)}(Y)}^2_2 + 2\m{p}\hat{\sigma}^2(Y)
		\]
		
		Instead of using the $\widehat{\m{SPSE}}^{(M)}$ directly, we will use Mallow's $C_\m{p}$ instead, given by
		\[
			\m{C_p}^{(M)} \define \frac{1}{\widehat{{\sigma}^2}}\sum_{i=1}^n\curvb{y_i-\hat{y}^{(M)}_i}^2 - n + 2\m{p}
		\]
		Minimising this value is equivalent to the minimisation of the $\widehat{\m{SPSE}}^{(M)}$.
		% The proof is trivial, just biject it to a conext-free residue class, whose elements are open manifolds. %citation
	
	% subsection mallows-C_p

	\subsection{Simulated Annealing}
	\label{ssec:model-selec}
	
		
		% The proposed solution in \ref{ssec:mallows-C_p} is to reduce the number of predictors.
		We can now formulate the following minimisation problem.
		Let
		\[
			\s{H} \define \set[A\in\setpow(\Lambda)]{A \cup \set{\lambda_0}} \subset \setpow\curvb{\overline{\Lambda}}
		\]
		Then the model we want to select is a solution to
		\[
			\curvb{\Omega} \qquad \minimize_{M\in\s{H}} \qquad C_\m{p}^{(M)}
		\]
		\[
			\Omega \define \minimize \set[M\in\s{H}]{C_\m{p}^{(M)}}
		\]
		This problem is a discrete optimisation problem, which is in general np-hard. %citation
		Unfortunately, $\abs{\s{H}} = 2^{319}$, which is too large for a complete search.
		Therefore, we need a heuristic search algorithm that does not depend on further information of the cost function.

		Simulated annealing (SANN) is a heuristic algorithm that approximates global optima of a given function.
		Note that the SANN returns only a probabilistically good approximation $\s{M}\in\s{H}$ to a true solution to $\Omega$.
		Lacking similarly reliable and computationally feasible alternatives we use the solution returned by SANN instead.
		% Quelle: https://en.wikipedia.org/wiki/Simulated_annealing

		The algorithm is applicable to arbitrary sets, in our case $\s{H}$.
		It simulates the slow cooling of a thermodynamic system.
		Let $x_0\in\s{H}$ be the initial set of predictors, $T_0\in(0,\infty)$ be the initial temperature of the system and $i_\m{max}\in\SN$ be the maximal number of time steps.
		Then the algorithm requires the following functions:
		\begin{itemize}
			\item $\func{\m{cost}}{\s{H}}{\SR}$ \\
				Calculates the cost of a given predictor set.
			\item $\func{\m{temp}}{\SR\times\SN^2}{(0,\infty)}$\\
				Calculates the temperature according to the given initial temperature and time steps.
				It is a monotonically decreasing function in the second parameter.
			\item $\func{\m{nbr}}{\s{H}}{\s{H}}$ \\
				Generates a random neighbor of a given predictor set.
			\item $\func{\m{prob}}{\SR^2\times(0,\infty)}{[0,1]}$ \\
				Calculates the probability of changing the current set or state to the neighbor.
			\item $\m{rnd}(0,1)$ \\
				Returns a random number in the interval $[0,1]$.
		\end{itemize}
		The listing shows one variant of the pseudocode of the SANN algorithm. 

		\medskip
		\begin{tcolorbox}[colframe=black,colbacktitle=white,coltitle=black, attach boxed title to top center={yshift=-2mm},enhanced, titlerule=0.1pt, boxrule=0.5pt, arc=5pt,title=Listing:\quad SANN algorithm]
			\input{sec/sann-algorithm}
		\end{tcolorbox}
		\medskip

	% subsection simulated-annealing
		
	\subsection{Model Validation}
	\label{ssec:model-validation}
	
		% goodness of prediction/R^2 discussion
		To examine the models selected by SANN, we recur to the often used $R^2$ measure.
		For $M\in\s{H}$ it is given by
		\[
			\curvb{R^2}^{(M)} \define \frac {\sum_{i=1}^n \curvb{\hat{y}^{(M)}_i - \overline{y}}^2}{\sum_{i=1}^n \curvb{y_i - \overline{y}}^2} \in [0,1]
		\]
		where $\overline{y}$ is the arithmetic mean of $y_i$ for $i=1,\ldots,n$ and describes how much of the total variation of $y$ is explained by the model $M$.
		$\curvb{R^2}^{(M)} = 0$ is equivalent to no, $\curvb{R^2}^{(M)} = 1$ to full agreement of the model with the observations.
		We shall be note though, that the measure is increasing with $\abs{M}$.
		In contrast the actual upper bound of $R^2$ is lowered by the presence of measurement errors.
		We will therefore consult the correlation diagrams for each response variable observation vector and its respective prediction vector to complement our judgement based on the $R^2$ measure.

	% subsection model-validation
	
	\subsection{Simulation}
	\label{ssec:simulation}
	
		As the true $\m{SPSE}$ is unobservable, we need to assess how well our procedure minimises the true $\m{SPSE}$ in general.
		For limitation of data we have to resort to so called pseudo-observations of a response variable.
		These are collected in a random vector as follows
		
		\[
			\widetilde{Y} \define \hat{y}^{(\s{M})} + \varepsilon,\qquad \varepsilon \sim  \FN \curvb{0,\curvb{\widehat{\sigma^2}}^{(\s{M})}\idmat_n}
		\]

		As we have fixed the parameters for the random vector $\widetilde{Y}$ we can now calculate the true $\m{SPSE}$ for the model.

		we resort to simulation of pseudo response variable $\tilde{Y}^{(j)}, j \in \SN, j \leq 1000$, where we can fix both, $\expect{\tilde{Y}^{(j)}}$ and $C^{(j)}$.
		The resulting values for the response variable are also called pseudo-observations.
		We then apply our model selection algorithm to each simulation and compare the resulting estimators $\widehat{\m{SPSE}}^{(j)}$ with the \enquote{true} $\m{SPSE}$ computed from the fixed parameters.
				
		The pseudo-observations of one simulation are generated as follows
		
		where $\hat{y}$ is the selected model from \ref{ssec:model-selec} and $\curvb{\widehat{\sigma^2}}^{(\s{M})}$ the estimator for $\sigma^2$ in said model.
		We then proceed to use the complete model space $\overline{\Lambda}$ to select the best model for each simulation and estimate the $\m{SPSE}$ through $\widehat{\m{SPSE}}^{(j)}$.
		We then compare the estimates with the fixed \enquote{true} $\m{SPSE}$.
		
		To gauge how much the resolution of the NIRS influences the model selection, we repeat the model selection on the simulations, but allow only models from
		\[
			\Lambda^{(m)} \define \set{\lambda_{mi} | \lambda_{mi} \in \Lambda, i = 1, 2, \ldots} \qquad m \in \set{2, 3, 4}
		\]
		where $\overline{\Lambda}^{(m)}$ takes the place of $\overline\Lambda$ in the algorithm.
		
		Just as with resolution, we can also assess the influence of availability of measurements.
		Choosing a suitable $N \in \SN^L$ with entries $N_l > k, l \in \set{1,\ldots,L}$, we randomly select a subset of the simulation data of size $N_l$ and proceed as above.
		As $n-k \le k$, we perform this inquiry with $\Lambda^{(3)}$ instead to reduce computational cost and capture a larger variety of L.
		
		Using the same algorithm for all three response variables, we can limit the algorithm examination to one response variable, $\m{SOC}$, to be better able to compare the results and reduce redundant analyses.		
		
	% subsection simulation
% section methodology