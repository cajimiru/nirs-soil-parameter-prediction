\section{Methodology}
\label{sec:methodology}
	
	\subsection{Measured Data}
	\label{ssec:measured-data}
	
		%For the prediction of soil parameters it is inevitable one already got some soil spectra with corresponding directly measured soil parameters.
		%We are using a dataframe made by Don %citation
		%of the Heinrich von Th√ºnen-Institut Braunschweig.
		
		As a single spectrum contains overlapping information, it is necessary to determine both relevant wavelengths and the respective parameters to apply NIRS to practical problems.
		To select wavelengths and determine parameters we use an example data set, which contains $p^\m{(SOC)},p^\m{(N)},\m{pH}$ and wave reflectances of 319 wavelengths ranging from $1400 \unit{nm}$ to $2672 \unit{nm}$ by steps of $4 \unit{nm}$ for 533 samples.%citation

		We define $\Lambda$ as the set of all measured wavelengths. The reflectance $\varrho(\lambda)$ of a sample at a wavelength $\lambda \in \Lambda$ is recorded as
		\[
			-\lg \varrho(\lambda) = -\frac{\ln \varrho(\lambda)}{\ln 10}
		\]
		
		\begin{figure*}
			\centering
			\input{gp/soil-spec-rnd.tex}
			\caption{This figure shows near infrared soil spectra of six randomly chosen soil samples obtained from the used dataframe.}
			\label{fig:soil-spec-rnd}
		\end{figure*}

		
		Figure \ref{fig:soil-spec-rnd} shows six randomly chosen soil spectra in a diagram.
	
	% subsection measured-data

	\subsection{Statistical Model}
	\label{ssec:statistical-model}
	
		Let $n\in\SN$ be the soil sample count and $k\in\SN$ with $k\leq n$ the number of measured wavelengths.
		$\varrho_i$ shall be defined as soil spectrum of the $i$th sample for every $i\in\SN,i\leq n$.
		$\lambda_j$ is the $j$th measured wavelength for every $j\in\SN,j\leq k$, also called predictor.
		Then according to section \ref{ssec:measured-data} the measured reflectance values are $x_{ij}$ with
		\[
			x_{ij} \define -\lg \varrho_i(\lambda_j)
		\]
		for every $i,j\in\SN,i\leq n,j\leq k$.
		Through the following matrix it is possible to get an easier notation.
		\[
			X \define (x_{ij}) \in \SR^{n\times k}
		\]
		Let $p^\m{(SOC)}_i, p^\m{(N)}_i, \m{pH}_i$ be the measured soil parameters of the $i$th sample for every $i\in\SN,i\leq n$, also known as observables.
		Then we define the $n$-dimensional vectors
		\begin{alignat*}{3}
			p^\m{(SOC)} &\define&&\ \curvb{p^\m{(SOC)}_i} \\
			p^\m{(N)} &\define&&\ \curvb{p^\m{(N)}_i} \\
			\m{pH} &\define&&\ \curvb{\m{pH}_i}
		\end{alignat*}

		The Beer-Lambert law allows us to make assumptions to the relations of soil spectra and soil parameters.
		In section \ref{ssec:nirs} we saw that the logarithmized reflectance can be written as a linear combination of molar concentrations.
		Hence, vice versa it makes sense to assume that an ASF can be represented by a linear combination of logarithmized reflectance values.

		Now let $P^\m{(SOC)},P^\m{(N)}$ be the appropriate random variables to the soil parameters $p^\m{(SOC)}$ and $p^{(N)}$.
		Then with the above assumption the expected values are given by
		\begin{alignat*}{3}
			\expect P^\m{(SOC)} &=&&\ X\beta^\m{(SOC)}_k + \beta^\m{(SOC)}_0 \definedby \mathbb{X}\beta^\m{(SOC)} \\
			\expect P^\m{(N)} &=&&\ X\beta^\m{(N)}_k + \beta^\m{(N)}_0 \definedby \mathbb{X}\beta^\m{(N)}
		\end{alignat*}
		When $\m{PH}$ is the corresponding random variable to $\m{pH}$ we have to perform a correction because $\m{pH}$ is a logarithmized molar concentration.
		It makes sense to model these with the subsequent expected value.
		\[
			\expect \m{PH} = \ln(X)\beta^\m{(pH)}_k + \beta^\m{(pH)}_0 \definedby \mathbb{X}_{\ln}\beta^\m{(pH)}
		\]

		In physics and chemistry it is a common and error-proven method to assume a normal distribution with a certain variance for measuring errors.
		So with the variances $(\sigma^2)^\m{(SOC)},(\sigma^2)^\m{(N)},(\sigma^2)^\m{(pH)}\in(0,\infty)$ our random variables become
		\begin{alignat*}{3}
			P^\m{(SOC)} &\sim&&\ \FN\curvb{\mathbb{X}\beta^\m{(SOC)}, (\sigma^2)^\m{(SOC)}\idmat_n} \\
			P^\m{(N)} &\sim&&\ \FN\curvb{\mathbb{X}\beta^\m{(N)}, (\sigma^2)^\m{(N)}\idmat_n} \\
			\m{PH} &\sim&&\ \FN\curvb{\mathbb{X}_{\ln}\beta^\m{(pH)}, (\sigma^2)^\m{(pH)}\idmat_n}
		\end{alignat*}
	
	% subsection statistical-model

	\subsection{Multivariate Linear Regression}
	\label{ssec:mlr}
	
		Multiple linear regression (MLR) or multivariate linear regression is a statistical method for estimating parameters that depend on linear independent variables.
		Let $\mathbb{X}\in\SR^{n\times(k+1)}, n,k\in\SN,k<n$ be the design matrix, $\sigma^2\in(0,\infty)$ and $Y$ be the vector of random variables with
		\[
			Y \sim \FN\curvb{\mathbb{X}\beta,\sigma^2\idmat_n}
		\]
		for a certain $\beta\in\SR^{k+1}$
		Then through the maximum-likelihood-method and a small correction we get two best unbiased estimators $\hat{\beta},\hat{\sigma^2}$ for $\beta$ and $\sigma^2$
		\begin{alignat*}{3}
			\hat{\beta}(Y) &=&&\ \inv{\curvb{\transp{\mathbb{X}}\mathbb{X}}}\transp{\mathbb{X}}Y \\
			\hat{\sigma^2}(Y) &=&&\ \frac{1}{n-(k+1)}\norm{Y - \mathbb{X}\hat{\beta}(Y)}^2
		\end{alignat*}
		Now let $y \define (y_i)\in\SR^n$ be a realization of $Y$.
		In this case we define
		\begin{alignat*}{3}
			\hat{y} &\define&&\ \mathbb{X}\hat{\beta}(y) = \mathbb{X}\inv{\curvb{\transp{\mathbb{X}}\mathbb{X}}}\transp{\mathbb{X}}y \\
			\hat{\sigma^2} &\define&&\ \hat{\sigma^2}(y)
		\end{alignat*}
		For more information please refer to [Quelle:Skript,wikipedia].
	
	% subsection mlr

	\subsection{Mallows' $\m{Cp}$}
	\label{ssec:mallows-cp}
	
		According to sections \ref{ssec:statistical-model} and \ref{ssec:mlr} at this time we are using $k+1 = 320$ predictors for our prediction model.
		Estimating Model Parameters with this large amount of predictors tends to overfit the measured data.
		% Quelle: Tutorial, Skript
		So it would be sensible to choose a \enquote{good} subset of predictors
		\[
			M\subset \set[i\in\SN_0,i\leq k]{\lambda_i}
		\]
		where $\lambda_0$ stands for the defined offset.
		Now through $M$ one can define a new design matrix $\mathbb{X}^{(M)}$.
		Applying MLR to this design matrix gives us new estimators
		\begin{alignat*}{3}
			\hat{\beta}^{(M)}(Y) &=&&\ \inv{\curvb{\transp{\mathbb{X}^{(M)}}\mathbb{X}^{(M)}}}\transp{\mathbb{X}^{(M)}}Y \\
			\hat{\sigma^2}^{(M)}(Y) &=&&\ \frac{1}{n-\#M}\norm{Y - \mathbb{X}^{(M)}\hat{\beta}^{(M)}(Y)}^2
		\end{alignat*}
		
		The term \enquote{good} refers to a criterion by which we can define the \enquote{goodness} of $M$.
		Here we will use Mallows' $\m{Cp}$.
		\[
			\m{Cp}^{(M)} \define \frac{1}{\hat{\sigma^2}}\sum_{i=1}^n\curvb{y_i-\hat{y}^{(M)}_i}^2 - n + 2\#M
		\]
		The minimization this value is equivalent to the minimization of the sum of predicted squared errors (SPSE).
	
	% subsection mallows-cp

	\subsection{Simulated Annealing}
	\label{ssec:simulated-annealing}
	
		The set of predictors contains $k=319$ free selectable elements (the constant shall remain).
		Therefore the power set $\s{P}$, the set of possible subsets $M$, contains of $2^{k}=2^{319}$ elements.
		If we want to find a subset $M$ so that for all $N\in\s{P}$ the inequation holds
		\[
			\m{Cp}^{(M)} \leq \m{Cp}^{(N)}
		\]
		we have to calculate $\m{Cp}^{(N)}$ for every $N\in\s{P}$.
		But this is a calculation beyond our current computing power.

		Simulated annealing (SA) is a probabilistic technique for approximating the global optimum of a given function.
		Specifically, it is a metaheuristic to approximate global optimization in a large search space.
		It simulates the slow cooling of a thermodynamic system through random numbers.
		With this algorithm it is possible to find a \enquote{good} local minimum in a short time.
		% Quelle: https://en.wikipedia.org/wiki/Simulated_annealing

		The algorithm works on an arbitrary set, in our case $\s{P}$.
		Let $x_0\in\s{P}$ be the initial set of predictors, $T_0\in(0,\infty)$ be the initial temperature of the system and $i_\m{max}\in\SN$ be the maximal number of time steps.
		Then the algorithm needs certain functions.
		\begin{itemize}
			\item $\func{\m{cost}}{\s{P}}{\SR}$ \\
				Calculates the cost of a given predictor set.
			\item $\func{\m{temp}}{\SR\times\SN^2}{(0,\infty)}$\\
				Calculates the temperature according to the given initial temperature and time steps.
				It is a monotonically decreasing function in the second parameter.
			\item $\func{\m{nbr}}{\s{P}}{\s{P}}$ \\
				Generates a random neighbor of a given predictor set.
			\item $\func{\m{prob}}{\SR^2\times(0,\infty)}{[0,1]}$ \\
				Calculates the probability of changing the current set or state to the neighbor.
			\item $\m{rnd}(0,1)$ \\
				Returns a random number in the interval $[0,1]$.
		\end{itemize}
		The listing shows one variant of the pseudocode of the SA algorithm. 

		\medskip
		\begin{tcolorbox}[colframe=black,colbacktitle=white,coltitle=black, attach boxed title to top center={yshift=-2mm},enhanced, titlerule=0.1pt, boxrule=0.5pt, arc=5pt,title=Listing:\quad SA algorithm]
			\input{sec/sa-algorithm}
		\end{tcolorbox}
		\medskip

	
	% subsection simulated-annealing

	\subsection{Model Validation}
	\label{ssec:model-validation}
	
		% goodness of prediction
	
	% subsection model-validation

% section methodology